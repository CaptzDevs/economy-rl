

async function runDQN(epochs = 3) {
  for (let epoch = 0; epoch < epochs; epoch++) {
    const sum = [];
    const rewards = [];
    let oldMoney = 0;
    let isFirstTrain = true;
    try {
      console.log("üíæ Loading Exist model...");
      await sleep(3000);
      await loadModel(); // ‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏¥‡∏° ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
      isFirstTrain = false;
    } catch {
      console.log("üì¶ No Existing Model => Create New");
      await sleep(3000);
      initDQN(); // ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠
    }

    for (let index = 0; index < 1; index++) {
      const epsilon = Math.max(0.1, 1.0 - index * 0.2); // üîª ‡∏•‡∏î epsilon

      citizens.forEach((c) => {
        c.strategy = "dqn";
        c.state.hunger = 100;
        c.state.energy = 100;
        c.state.happiness = 100;
        c.state.health = 100;
        c.money = index === 0 ? 100 : oldMoney;
        c.inventory.food = 1;
        c.alive = true;
        c.memory.logs = [];
        c.actionIndex = 0;
        c.totalReward = 0;
        c.epsilon = epsilon;
        c.age = 1;
      });

      console.log(
        `üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° Simulation ‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà ${index + 1} (Œµ = ${epsilon.toFixed(2)})`
      );

      const trainedTick = await runSimulation(1000, 0, (tick) => {
        if (tick % 10 === 0) {
          citizens.forEach((c) => c.age++);
        }
      });

      sum.push(citizens[0].age);
      rewards.push(citizens[0].totalReward);
      oldMoney = citizens[0].money;
    }

    // üìä ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏£‡∏ß‡∏°
    console.log("\n\n-----------Result-----------");
    console.log("ü§ñ ‡∏à‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß");
    console.log("üìà ‡∏≠‡∏≤‡∏¢‡∏∏‡∏£‡∏ß‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö:", sum);
    console.log("ü•á Reward ‡∏£‡∏ß‡∏°‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö:", rewards);
    if ((await compareModelPerformance({ sum, rewards })) || isFirstTrain) {
      // üíæ Save Model
      await saveModel();
      console.log("ü§ñ Saved Model");
    }

    // üß† ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ agent
    citizens.forEach((c) => {
      console.log(`${c.name} age: ${c.age} reword : ${c.totalReward}`);
      /* console.log(`${c.name} action summary:`, summarizeActions(c.memory.logs)); */
    });
    await sleep(5000);
  }
}

/* runDQN(20); */

const compareModelPerformance = async (newResult) => {
  try {
    const result = await loadJSON(result_path);

    console.log(newResult.map((item) => item.age));
    console.log(newResult.map((item) => item.totalReward));

    const totalAge = result.sum.reduce((a, b) => a + b, 0);
    const totalNewAge = newResult.sum.reduce((a, b) => a + b, 0);
    const totalReward = result.rewards.reduce((a, b) => a + b, 0);
    const totalNewReward = newResult.rewards.reduce((a, b) => a + b, 0);

    console.log("\n\n---------Compare Model Performance-----------");

    console.log("üìà Old Age:", result.sum);
    console.log("ü•á Old Reward:", result.rewards);
    // ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡∏±‡∏ö‡πÉ‡∏´‡∏°‡πà

    if (totalNewAge > totalAge && totalNewReward > totalReward) {
      saveJSON(result_path, newResult);
      console.log(chalk.green("‚úÖ Model result : Better"));
      return true;
    } else {
      console.log(chalk.red("‚ùå Model result : Bad"));

      return false;
    }
  } catch (err) {
    // ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏ü‡∏•‡πå performance ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏°‡πà‡∏°‡∏µ => ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà
    saveJSON(result_path, newResult);
    console.log("üìÑ No Model Result : Create new ");
    return false;
  }
};

async function runNN() {
  // 0. ‡πÉ‡∏ä‡πâ behavior tree ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ log
  const sum = [];
  const logs = [];
  citizens.forEach((c) => (c.strategy = "dqn"));

  // 1. ‡πÄ‡∏Å‡πá‡∏ö logs
  const trainedTick = await runSimulation(1000, 0);
  sum.push(trainedTick);

  const trainedLogs = citizens[0].memory.logs;
  for (let index = 0; index < 5; index++) {
    const logs = citizens[0].memory.logs;
    const result = await trainFromLogs(trainedLogs);

    if (!result) {
      console.error("‚ùå ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (log ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏û‡∏≠)");
      return;
    }

    const { model, ACTIONS } = result;
    // 3. ‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
    setTrainedModel(model, ACTIONS);

    // 4. ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï agent
    citizens.forEach((c) => {
      c.strategy = "nn"; // ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡πâ‡∏ß
      c.state.hunger = 100;
      c.state.energy = 100;
      c.state.happiness = 100;
      c.state.health = 100;
      c.money = 100;
      c.inventory.food = 1;
      c.alive = true;
      c.memory.logs = [];
      c.actionIndex = 0;
    });

    // 5. ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
    console.log(`\nü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß`);
    sum.push(await runSimulation(1000, 0));
  }
  console.log("\n\nü§ñ ‡∏à‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß", sum);
  console.log(
    "\n\nü§ñ AGV.",
    (sum.reduce((a, b) => a + b, 0) / sum.length).toFixed(2)
  );

  console.log(summarizeActions(trainedLogs));
}

/* runNN(); */

function summarizeActions(logs) {
  const summary = {};

  for (const log of logs) {
    const action = log.action;
    if (!action) continue;

    if (!summary[action]) {
      summary[action] = 0;
    }

    summary[action]++;
  }

  return summary;
}
